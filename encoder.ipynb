{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\cyborg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = []\n",
    "for filename in os.listdir('data'):\n",
    "  with open(os.path.join('data', filename), encoding='latin-1') as f:\n",
    "    corpus = f.read()\n",
    "  raw_sent = sent_tokenize(corpus)\n",
    "  for sent in raw_sent:\n",
    "    story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145020"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6569624, 8628190)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stormborn', 0.8237999677658081),\n",
       " ('unburnt', 0.7762250900268555),\n",
       " ('targaryen', 0.7697181105613708),\n",
       " ('queen', 0.6973780393600464),\n",
       " ('elia', 0.6915681958198547),\n",
       " ('myrcella', 0.6823866963386536),\n",
       " ('princess', 0.6806121468544006),\n",
       " ('margaery', 0.6693328619003296),\n",
       " ('viserys', 0.6624032258987427),\n",
       " ('khal', 0.6531725525856018)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('daenerys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super().__init__();\n",
    "\n",
    "        self.wq = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.wk = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.wv = nn.Linear(num_features, num_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        energy = torch.matmul(q, k.T) * x.shape[1] ** -0.5\n",
    "        # energy.tril_()\n",
    "        # energy[energy==0] = float('-inf')\n",
    "        # energy = energy.masked_fill(self.tril)\n",
    "        # mask = torch.full((energy.shape[0], energy.shape[1]), float('-inf'))\n",
    "        # mask = torch.triu(mask, diagonal=1)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, v)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(num_features=num_features) for _ in range(num_heads)])\n",
    "        self.wo = nn.Linear(num_features * num_heads, num_features, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        out = self.wo(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_features),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, total_tokens):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe = torch.zeros((total_tokens, d_model))\n",
    "        self.d_model = d_model\n",
    "        self.total_tokens = total_tokens\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for pos in range(self.total_tokens):\n",
    "\n",
    "            for i in range(self.d_model // 2):\n",
    "\n",
    "                theta = torch.tensor(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "                self.pe[pos, 2 * i] = torch.sin(theta) \n",
    "                self.pe[pos, 2 * + 1] = torch.cos(theta)\n",
    "\n",
    "        x = x + self.pe \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, num_features, total_tokens, x):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = x\n",
    "        self.mha = MultiHeadAttention(num_heads, num_features)\n",
    "        self.pe = PositionalEncoding(num_features, total_tokens)\n",
    "        self.ffwd = FeedForward(num_features)\n",
    "        self.ln1 = nn.LayerNorm(num_features)\n",
    "        self.ln2 = nn.LayerNorm(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ln2(self.ffwd(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor(model.wv.get_normed_vectors())\n",
    "\n",
    "# block = Block(2, x.shape[1], x.shape[0], x)\n",
    "# x = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, total_tokens, num_heads, x):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe = PositionalEncoding(d_model=num_features, total_tokens=total_tokens)\n",
    "        self.ffwd = FeedForward(num_features)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(num_heads, num_features, total_tokens, x=x),\n",
    "            Block(num_heads, num_features, total_tokens, x=x)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pe(x)\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(model.wv.get_normed_vectors())\n",
    "\n",
    "encoder = Encoder(num_features=x.shape[1], total_tokens=x.shape[0], num_heads=2, x=x)\n",
    "\n",
    "x = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17453, 100])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=float('inf'))\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
