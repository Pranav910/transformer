{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\cyborg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = []\n",
    "for filename in os.listdir('data'):\n",
    "  with open(os.path.join('data', filename), encoding='latin-1') as f:\n",
    "    corpus = f.read()\n",
    "  raw_sent = sent_tokenize(corpus)\n",
    "  for sent in raw_sent:\n",
    "    story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145020"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6571778, 8628190)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stormborn', 0.8051601648330688),\n",
       " ('targaryen', 0.7532665729522705),\n",
       " ('unburnt', 0.7423449754714966),\n",
       " ('myrcella', 0.723058819770813),\n",
       " ('princess', 0.7199079990386963),\n",
       " ('elia', 0.6827012896537781),\n",
       " ('dorne', 0.6826499104499817),\n",
       " ('martell', 0.6766705513000488),\n",
       " ('queen', 0.6484774947166443),\n",
       " ('aegon', 0.6469336748123169)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('daenerys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super().__init__();\n",
    "\n",
    "        self.wq = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.wk = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.wv = nn.Linear(num_features, num_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        energy = torch.matmul(q, k.T) * x.shape[1] ** -0.5\n",
    "        energy.tril_()\n",
    "        energy[energy==0] = float('-inf')\n",
    "        # energy = energy.masked_fill(self.tril)\n",
    "        # mask = torch.full((energy.shape[0], energy.shape[1]), float('-inf'))\n",
    "        # mask = torch.triu(mask, diagonal=1)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, v)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(num_features=num_features) for _ in range(num_heads)])\n",
    "        self.wo = nn.Linear(num_features * num_heads, num_features, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        out = self.wo(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_features),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodeing(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, total_tokens):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe = torch.zeros((total_tokens, d_model))\n",
    "        self.d_model = d_model\n",
    "        self.total_tokens = total_tokens\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for pos in range(self.total_tokens):\n",
    "\n",
    "            for i in range(self.d_model// 2):\n",
    "\n",
    "                theta = torch.tensor(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "                self.pe[pos, 2 * i] = torch.sin(theta) \n",
    "                self.pe[pos, 2 * + 1] = torch.cos(theta)\n",
    "\n",
    "        x = x + self.pe \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, num_features, total_tokens, x):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = x\n",
    "        self.mha = MultiHeadAttention(num_heads, num_features)\n",
    "        self.pe = PositionalEncodeing(num_features, total_tokens)\n",
    "        self.ffwd = FeedForward(num_features)\n",
    "        self.ln = nn.LayerNorm(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pe(x) + x\n",
    "        ox = x\n",
    "        x = self.mha(x) + ox\n",
    "        normed_x = self.ln(x)\n",
    "        x = self.ffwd(normed_x) + normed_x\n",
    "        x = self.ln(x) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(model.wv.get_normed_vectors())\n",
    "\n",
    "block = Block(2, x.shape[1], x.shape[0], x)\n",
    "x = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=float('inf'))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
